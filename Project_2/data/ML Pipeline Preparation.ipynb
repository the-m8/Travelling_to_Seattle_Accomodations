{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sqlite3\n",
    "import pickle\n",
    "from sqlalchemy import create_engine\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4  12  says: west side of Haiti, rest of the country ...   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
       "4  facade ouest d Haiti et le reste du pays aujou...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products      ...        \\\n",
       "0        0      0            0             0                 0      ...         \n",
       "1        0      0            1             0                 0      ...         \n",
       "2        0      0            0             0                 0      ...         \n",
       "3        1      0            1             0                 1      ...         \n",
       "4        0      0            0             0                 0      ...         \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "2            0                     0                0       0      0     0   \n",
       "3            0                     0                0       0      0     0   \n",
       "4            0                     0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "2           0     0              0              0  \n",
       "3           0     0              0              0  \n",
       "4           0     0              0              0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterResponseData.db')\n",
    "df = pd.read_sql('SELECT * FROM \"disaster_response\"', engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    \"\"\"\n",
    "    Loading transformed data of Disaster Response and split into X and y.\n",
    "  \n",
    "    This function connects to the sqlite server where the ETL-Disaster Response Data is stored, loads the dataset, \n",
    "    removes the column \"id\" and splits the remaining columns into messages (X) and number values (y).\n",
    "      \n",
    "    Parameters:\n",
    "    None\n",
    "  \n",
    "    Returns:\n",
    "    array (X, y): messages and number values.\n",
    "  \n",
    "    \"\"\"\n",
    "    \n",
    "    engine = create_engine('sqlite:///DisasterResponseData.db')\n",
    "    df = pd.read_sql('SELECT * FROM \"disaster_response\"', engine)\n",
    "    df.drop(columns=['id'], inplace=True)\n",
    "    X = df.message.values\n",
    "    y = df.select_dtypes(include=np.number).values\n",
    "    category_names = list(df.select_dtypes(include=np.number).columns)\n",
    "    return X, y, category_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, category_names = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['related',\n",
       " 'request',\n",
       " 'offer',\n",
       " 'aid_related',\n",
       " 'medical_help',\n",
       " 'medical_products',\n",
       " 'search_and_rescue',\n",
       " 'security',\n",
       " 'military',\n",
       " 'child_alone',\n",
       " 'water',\n",
       " 'food',\n",
       " 'shelter',\n",
       " 'clothing',\n",
       " 'money',\n",
       " 'missing_people',\n",
       " 'refugees',\n",
       " 'death',\n",
       " 'other_aid',\n",
       " 'infrastructure_related',\n",
       " 'transport',\n",
       " 'buildings',\n",
       " 'electricity',\n",
       " 'tools',\n",
       " 'hospitals',\n",
       " 'shops',\n",
       " 'aid_centers',\n",
       " 'other_infrastructure',\n",
       " 'weather_related',\n",
       " 'floods',\n",
       " 'storm',\n",
       " 'fire',\n",
       " 'earthquake',\n",
       " 'cold',\n",
       " 'other_weather',\n",
       " 'direct_report']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Transforming string text into arrays of words.\n",
    "  \n",
    "    This function splits strings into words, removes stopwords and lemmatize the words.\n",
    "      \n",
    "    Parameters:\n",
    "    text (string): string input containing text.\n",
    "  \n",
    "    Returns:\n",
    "    lemmed (array): array with separate text strings for machine learning model input.\n",
    "  \n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = word_tokenize(text.lower().strip())\n",
    "    tokens = [w for w in tokens if w not in stopwords.words(\"english\")]\n",
    "    lemmed = [WordNetLemmatizer().lemmatize(w) for w in tokens]\n",
    "    return lemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weather', 'update', '-', 'cold', 'front', 'cuba', 'could', 'pas', 'haiti']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline():\n",
    "    \"\"\"\n",
    "    Machine learning pipeline to predict classification.\n",
    "  \n",
    "    This function contains a pipeline where the data input gets vectorized, transformed and the output gets predicted with\n",
    "    KNeighboursClassifier.\n",
    "      \n",
    "    Parameters:\n",
    "    None.\n",
    "  \n",
    "    Returns:\n",
    "    Pipeline.\n",
    "  \n",
    "    \"\"\"\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(KNeighborsClassifier()))\n",
    "    ])\n",
    "    return pipeline\n",
    "pipeline = model_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y, category_names = load_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-09c4c16b5509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                        **fit_params):\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-de0b41490dbb>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mlemmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlemmed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-de0b41490dbb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mlemmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlemmed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[1;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/corpus/reader/api.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \"\"\"\n\u001b[1;32m    212\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, encoding)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeekableUnicodeStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, stream, encoding, errors)\u001b[0m\n\u001b[1;32m   1110\u001b[0m            beginning of ``linebuffer`` (which is required by ``tell()``).\"\"\"\n\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_bom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m         \"\"\"The length of the byte order marker at the beginning of\n\u001b[1;32m   1114\u001b[0m            the stream (or None for no byte order marker).\"\"\"\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_check_bom\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1466\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbom_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m             \u001b[0;31m# Read a prefix, to check against the BOM(s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1468\u001b[0;31m             \u001b[0mbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1469\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(y_test, y_pred, category_names):\n",
    "    \n",
    "    \"\"\"\n",
    "    Displaying model results.\n",
    "  \n",
    "    This function displays the f1 score, precision and recall in the classification report of the predicted output.\n",
    "    It shows how well the selected prediction model works.\n",
    "      \n",
    "    Parameters:\n",
    "    y_test: actual results from the dataset, created within train_test_split.\n",
    "    y_pred: predicted results in Model.\n",
    "    category_names: category names in Dataset, column names of y_pred.\n",
    "  \n",
    "    Returns:\n",
    "    classification report: shows precision, recall, f1-score and support on different target names.\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    for idx, column in enumerate(category_names):\n",
    "        print(column)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'clf__estimator__leaf_size': [20, 30, 40]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'clf__estimator__leaf_size': [20, 30, 40],\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid = parameters, verbose = True)\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 95.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters: {'clf__estimator__leaf_size': 20}\n"
     ]
    }
   ],
   "source": [
    "cv.fit(X_train, y_train)\n",
    "y_pred = cv.predict(X_test)\n",
    "print(\"\\nBest Parameters:\", cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target names: [0 1 2]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.99      0.96    215232\n",
      "          1       0.78      0.34      0.47     20651\n",
      "          2       0.83      0.16      0.27        61\n",
      "\n",
      "avg / total       0.93      0.93      0.92    235944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_results(y_test, y_pred, category_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_rfc():\n",
    "    \n",
    "    \"\"\"\n",
    "    Machine learning pipeline to predict classification.\n",
    "  \n",
    "    This function contains a pipeline where the data input gets vectorized, transformed and the output gets predicted with\n",
    "    RandomForestClassifier.\n",
    "      \n",
    "    Parameters:\n",
    "    None.\n",
    "  \n",
    "    Returns:\n",
    "    Pipeline.\n",
    "  \n",
    "    \"\"\"\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "            ('vec', CountVectorizer(tokenizer=tokenize)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "        ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x7fe29e24d9d8>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "              metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "              weights='uniform'),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x7fe29e24d9d8>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "            weights='uniform'),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__algorithm': 'auto',\n",
       " 'clf__estimator__leaf_size': 30,\n",
       " 'clf__estimator__metric': 'minkowski',\n",
       " 'clf__estimator__metric_params': None,\n",
       " 'clf__estimator__n_jobs': 1,\n",
       " 'clf__estimator__n_neighbors': 5,\n",
       " 'clf__estimator__p': 2,\n",
       " 'clf__estimator__weights': 'uniform',\n",
       " 'clf__estimator': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "            weights='uniform'),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_rfc():\n",
    "    \n",
    "    \"\"\"\n",
    "    Machine learning pipeline with GridSearch to predict classification.\n",
    "  \n",
    "    This function contains a pipeline where the data input gets vectorized, transformed and the output gets predicted with\n",
    "    RandomForestClassifier. The pipeline gets modified by GridSearch to look for the best parameter values.\n",
    "      \n",
    "    Parameters:\n",
    "    None.\n",
    "  \n",
    "    Returns:\n",
    "    Pipeline.\n",
    "  \n",
    "    \"\"\"\n",
    "    pipeline = Pipeline([\n",
    "            ('vec', CountVectorizer(tokenizer=tokenize)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', MultiOutputClassifier(RandomForestClassifier(n_estimators=5)))\n",
    "        ])\n",
    "    \n",
    "    parameters = {'clf__estimator__max_features':['sqrt', 0.5]}\n",
    "\n",
    "    cv = GridSearchCV(pipeline, param_grid = parameters, cv = 2, n_jobs = 6, verbose = True)\n",
    "\n",
    "    return cv\n",
    "cv_rfc = model_pipeline_rfc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   4 out of   4 | elapsed: 15.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters: {'clf__estimator__max_features': 0.5}\n"
     ]
    }
   ],
   "source": [
    "cv_rfc.fit(X_train, y_train)\n",
    "y_pred_rfc = cv_rfc.predict(X_test)\n",
    "print(\"\\nBest Parameters:\", cv_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "request\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "offer\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "aid_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "medical_help\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "medical_products\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "search_and_rescue\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "security\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "military\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "child_alone\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "water\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "food\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "shelter\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "clothing\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "money\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "missing_people\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "refugees\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "death\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "other_aid\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "infrastructure_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "transport\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "buildings\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "electricity\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "tools\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "hospitals\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "shops\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "aid_centers\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "other_infrastructure\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "weather_related\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "floods\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "storm\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "fire\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "earthquake\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "cold\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "other_weather\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n",
      "direct_report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.91      0.87      4978\n",
      "          1       0.67      0.54      0.60      1147\n",
      "          2       0.00      0.00      0.00        32\n",
      "          3       0.70      0.66      0.68      2735\n",
      "          4       0.48      0.30      0.37       519\n",
      "          5       0.54      0.33      0.41       339\n",
      "          6       0.39      0.22      0.28       191\n",
      "          7       0.20      0.07      0.10       127\n",
      "          8       0.43      0.27      0.33       210\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.73      0.69      0.71       429\n",
      "         11       0.76      0.76      0.76       714\n",
      "         12       0.69      0.63      0.66       577\n",
      "         13       0.69      0.64      0.66        96\n",
      "         14       0.49      0.26      0.34       147\n",
      "         15       0.36      0.25      0.29        76\n",
      "         16       0.51      0.27      0.36       250\n",
      "         17       0.61      0.56      0.58       297\n",
      "         18       0.38      0.20      0.26       854\n",
      "         19       0.25      0.09      0.13       455\n",
      "         20       0.39      0.29      0.33       290\n",
      "         21       0.58      0.38      0.46       345\n",
      "         22       0.56      0.37      0.45       129\n",
      "         23       0.08      0.02      0.04        42\n",
      "         24       0.26      0.12      0.16        76\n",
      "         25       0.00      0.00      0.00        34\n",
      "         26       0.26      0.09      0.13        88\n",
      "         27       0.19      0.05      0.08       295\n",
      "         28       0.79      0.75      0.77      1832\n",
      "         29       0.75      0.58      0.65       536\n",
      "         30       0.69      0.70      0.69       608\n",
      "         31       0.59      0.39      0.47        84\n",
      "         32       0.86      0.81      0.83       617\n",
      "         33       0.56      0.44      0.50       140\n",
      "         34       0.39      0.22      0.28       337\n",
      "         35       0.61      0.44      0.51      1300\n",
      "\n",
      "avg / total       0.67      0.61      0.63     20926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_results(y_test, y_pred_rfc, category_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = model_pipeline_rfc()\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(final_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
